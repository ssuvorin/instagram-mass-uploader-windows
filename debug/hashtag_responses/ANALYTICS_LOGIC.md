# Логика сбора аналитики по хештегам Instagram

## Обзор процесса

Процесс сбора аналитики состоит из нескольких этапов:
1. Аутентификация и получение информации о хештеге
2. Пагинация по медиа через эндпоинт sections
3. Извлечение медиа из ответов (обычные посты и clips/reels)
4. Получение просмотров через media_info API для недостающих медиа
5. Агрегация данных (просмотры, лайки, комментарии)

---

## Эндпоинты Instagram API

### 1. Получение информации о хештеге
**Эндпоинт:** `GET /api/v1/tags/{hashtag}/info/`
**Метод:** `cl.hashtag_info(hashtag)`
**Что получаем:**
- `media_count` - общее количество медиа в хештеге (51 для #cinemanv6)
- Базовую информацию о хештеге

**Когда вызывается:**
- Один раз в начале анализа
- С повторами при ошибке `LoginRequired` (до 3 попыток)

---

### 2. Получение медиа по хештегу (пагинация)
**Эндпоинт:** `POST /api/v1/tags/{hashtag}/sections/`
**Метод:** `cl.private_request(f"tags/{hashtag}/sections/", data=data)`
**Параметры запроса:**
```json
{
  "media_recency_filter": "top_recent_posts",
  "_uuid": "<client_uuid>",
  "include_persistent": "false",
  "rank_token": "<rank_token>",
  "max_id": "<next_max_id>",           // Для пагинации
  "next_media_ids": "[<id1>, <id2>]"  // JSON строка с массивом ID
}
```

**Структура ответа:**
```json
{
  "sections": [
    {
      "layout_type": "media_grid" | "one_by_two_left",
      "feed_type": "media" | "clips",
      "layout_content": {
        "medias": [...],  // Обычные посты
        "one_by_two_item": {
          "clips": {
            "items": [...],      // Clips/Reels
            "fill_items": [...]  // Дополнительные clips
          }
        }
      }
    }
  ],
  "more_available": true,
  "next_max_id": "<string>",
  "next_media_ids": [<id1>, <id2>, ...],
  "next_page": 1,
  "status": "ok"
}
```

**Пагинация:**
- `next_max_id` - строка-токен для следующей страницы (base64 encoded `[next_max_id, next_media_ids]`)
- `next_media_ids` - массив ID медиа, которые Instagram планирует показать на следующей странице
- Пагинация продолжается пока `more_available: true` ИЛИ есть `next_max_id`

**Когда вызывается:**
- В цикле для каждой страницы (до `max_pages`, по умолчанию 50)
- С переменным размером страницы (20-35 медиа) для анти-детекции
- С задержками между запросами (1.5-4.5 сек)

---

### 3. Получение детальной информации о медиа
**Эндпоинт:** `GET /api/v1/media/{media_id}/info/`
**Метод:** `cl.media_info(media_id)`
**Что получаем:**
- `play_count` - просмотры для видео/reels
- `view_count` - просмотры (альтернативное поле)
- `video_view_count` - просмотры видео (альтернативное поле)
- `like_count` - количество лайков
- `comment_count` - количество комментариев
- `media_type` - тип медиа (2=video, 13=clip/reel)

**Когда вызывается:**
1. **Для недостающих медиа:**
   - Если медиа из `next_media_ids` не получены в секциях
   - Если секции пустые, но есть `next_media_ids`
   - Если ожидались ID из предыдущего запроса, но не получены

2. **Для медиа без просмотров:**
   - Если в извлеченном медиа нет `play_count`, `view_count`, `video_view_count`
   - Только для видео/reels (`media_type` в (2, 13))

3. **Для медиа без лайков/комментариев:**
   - Если `like_count` или `comment_count` не доступны в извлеченном медиа

---

## Процесс обработки медиа

### Этап 1: Извлечение медиа из секций

**Обычные посты (`feed_type: "media"`):**
```python
layout_content.get("medias") -> каждый node.get("media") -> extract_media_v1()
```

**Clips/Reels (`feed_type: "clips"`):**
```python
layout_content.get("one_by_two_item").get("clips") -> items + fill_items
каждый clip_item.get("media") -> extract_media_v1()
```

**Извлечение:**
- Используется `extract_media_v1()` из `instagrapi.extractors`
- Проверяется дедупликация по `media.id` или `media.pk`
- Сохраняются только уникальные медиа

### Этап 2: Проверка недостающих медиа

После обработки секций проверяем:
1. **Ожидаемые ID из предыдущего запроса** (`expected_media_ids_from_prev`)
2. **ID из текущего ответа** (`next_media_ids`), если:
   - Секции пустые (`sections_count == 0`)
   - ИЛИ не получены все ожидаемые ID

**Если найдены недостающие ID:**
- Запрашиваем через `media_info` API
- Добавляем в список медиа для анализа
- Сохраняем ответы в debug файлы

### Этап 3: Агрегация данных

Для каждого медиа проверяем:
1. **Просмотры:**
   - Сначала `play_count` (приоритет для reels)
   - Потом `view_count`
   - Потом `video_view_count`
   - Если нет - запрос через `media_info` (только для видео/reels)

2. **Лайки:**
   - `like_count` из извлеченного медиа
   - Если нет - запрос через `media_info`

3. **Комментарии:**
   - `comment_count` из извлеченного медиа
   - Если нет - запрос через `media_info`

**Результат:**
- `total_views` - сумма всех просмотров
- `total_likes` - сумма всех лайков
- `total_comments` - сумма всех комментариев
- `analyzed_medias` - количество медиа с просмотрами
- `fetched_medias` - общее количество полученных медиа

---

## Логика пагинации

### Алгоритм:

```
1. Запрос без max_id (первая страница)
   ↓
2. Получаем ответ с next_max_id и next_media_ids
   ↓
3. Декодируем max_id: [next_max_id, next_media_ids]
   ↓
4. Следующий запрос с:
   - max_id = decoded[0] (next_max_id)
   - next_media_ids = decoded[1] (массив ID)
   ↓
5. Повторяем пока:
   - more_available == true ИЛИ
   - next_max_id != null
   ↓
6. Останавливаемся если:
   - more_available == false И next_max_id == null
   - ИЛИ достигнут max_pages
   - ИЛИ обнаружен цикл (одни и те же next_media_ids 3 раза подряд)
```

### Особенности:

1. **Дедупликация медиа:**
   - Используется `processed_media_ids` set
   - Проверяется перед добавлением медиа
   - Пропускаются дубликаты с логированием

2. **Обработка пустых секций:**
   - Если секции пустые, но есть `next_media_ids` - запрашиваем их через `media_info`
   - Продолжаем пагинацию если есть `next_max_id`

3. **Обнаружение циклов:**
   - Отслеживаем повторяющиеся `next_media_ids`
   - Если одни и те же ID повторяются 3 раза подряд - останавливаемся

---

## Анти-детекция

1. **Задержки между запросами:**
   - Базовая задержка: 1.5-4.5 сек (`_human_delay()`)
   - Микро-паузы: случайные 0.3-0.8 сек (`_random_micro_pause()`)
   - Длинные паузы: каждые N страниц, 8-18 сек (`_human_long_think()`)

2. **Переменный размер страницы:**
   - Базовый размер: 27 медиа
   - Вариация: ±3 медиа (20-35)
   - Для имитации человеческого поведения

3. **Симуляция чтения контента:**
   - После обработки страницы с медиа: 2-5 сек задержка
   - Имитирует время на просмотр медиа

---

## Сохранение debug данных

### Структура файлов:

```
debug/hashtag_responses/
├── hashtag_{hashtag}_{timestamp}.json          # Summary файл (обновляется после каждой страницы)
└── hashtag_{hashtag}_{timestamp}_raw/          # Raw ответы
    ├── page_001_raw_response.json              # Полный ответ sections API
    ├── page_001_media_info_{media_id}.json     # Ответ media_info API
    ├── page_002_raw_response.json
    └── ...
```

### Что сохраняется:

1. **Summary файл:**
   - Данные запроса и ответа для каждой страницы
   - Список полученных медиа ID
   - Список медиа, запрошенных через media_info
   - Образцы raw данных медиа (первые 2)
   - Итоговая статистика

2. **Raw ответы:**
   - Полный ответ от `sections` API (все секции, все медиа)
   - Полный ответ от `media_info` API (если доступен)
   - Данные запроса (включая декодированный max_id)

---

## Примеры использования

### Пример 1: Обычная пагинация
```
Страница 1: sections пустые, но есть next_media_ids [A, B, C]
  → Запрашиваем A, B, C через media_info
  
Страница 2: sections содержат медиа A, next_media_ids [D]
  → Получаем A из секций, запрашиваем D через media_info
  
Страница 3: sections содержат медиа D, more_available: false
  → Получаем D из секций, пагинация завершена
```

### Пример 2: Пустые секции
```
Страница 1: sections пустые, next_media_ids [A, B, C]
  → Запрашиваем A, B, C через media_info
  
Страница 2: sections пустые, next_media_ids [A, B, C] (те же!)
  → Но next_max_id изменился → продолжаем пагинацию
  → Запрашиваем A, B, C снова (будут пропущены как дубликаты)
  
Страница 3: sections содержат медиа, next_media_ids [D]
  → Получаем медиа из секций, запрашиваем D через media_info
```

### Пример 3: Недостающие просмотры
```
Медиа получено из секций, но нет play_count
  → Проверяем media_type (2 или 13 = видео/reel)
  → Если видео/reel → запрашиваем через media_info
  → Получаем play_count из media_info ответа
```

---

## Обработка ошибок

1. **LoginRequired:**
   - До 3 попыток восстановления сессии
   - Если не удалось - останавливаем анализ

2. **ChallengeRequired:**
   - Немедленная остановка (требуется ручное вмешательство)

3. **PleaseWaitFewMinutes (rate limit):**
   - Задержка 60-120 сек
   - Повтор запроса

4. **Ошибки валидации медиа:**
   - Логирование ошибки
   - Пропуск проблемного медиа
   - Продолжение обработки остальных

5. **Ошибки media_info:**
   - Логирование ошибки
   - Пропуск проблемного медиа
   - Продолжение обработки остальных

---

## Итоговая статистика

В результате анализа получаем:
- `fetched_medias` - общее количество полученных медиа
- `analyzed_medias` - количество медиа с просмотрами
- `total_views` - сумма всех просмотров
- `total_likes` - сумма всех лайков
- `total_comments` - сумма всех комментариев
- `pages_loaded` - количество обработанных страниц
- `skipped_duplicates` - количество пропущенных дубликатов

Данные сохраняются в базу данных через `HashtagAnalytics` модель.

